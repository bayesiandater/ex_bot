{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31cb41b2-6ec5-4370-9ce9-a3d747f15feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import pickle\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f409910-8391-49cf-a4ff-aece5ef41018",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_from_ex = pickle.load(open(\"/Users/bayesiandater/ex_bot/resources/msg_from_ex\", \"rb\"))\n",
    "msg_from_ex = [m.lower() for m in msg_from_ex if m.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3529e6bd-dd71-4ce4-be60-32cfeaea8788",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(713)\n",
    "train_idx = rng.integers(low=0, high=len(msg_from_ex), size=int(0.8 * len(msg_from_ex)))\n",
    "valid_idx = [i for i in range(len(msg_from_ex)) if i not in train_idx]\n",
    "train_ls, valid_ls = np.array(msg_from_ex)[train_idx], np.array(msg_from_ex)[valid_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b5e7f-a576-41b3-bba7-7110a6b54685",
   "metadata": {},
   "source": [
    "#### decide on the number of sentence length to feed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c83f29-b614-4f64-959b-bfd5c88558f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([54., 27., 24., 14.,  3.,  3.,  0.,  1.,  0.,  1.]),\n",
       " array([ 1. ,  3.1,  5.2,  7.3,  9.4, 11.5, 13.6, 15.7, 17.8, 19.9, 22. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(m.strip().split()) for m in train_ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eebef3a-f290-4f20-aa98-17e50d8ff9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the custom tokens to add to tokenizer\n",
    "# get the GPT2 token list\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "GPT2_tokens = []\n",
    "for w in tokenizer.encoder.keys():\n",
    "    w = w.strip(\"Ä \").lower()\n",
    "    GPT2_tokens.append(w)\n",
    "    \n",
    "# only add all whole words from both train + valid sets \n",
    "# that are not in the original GPT2 tokenizer's vocabulary 15\n",
    "all_words = []\n",
    "for m in msg_from_ex:\n",
    "    m = m.strip(string.punctuation).lower().split()\n",
    "    all_words += m\n",
    "\n",
    "added_tokens = []\n",
    "for word, freq in Counter(all_words).most_common():\n",
    "    if word not in GPT2_tokens:\n",
    "        added_tokens.append(word)\n",
    "        \n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25ad6e75-6a54-475a-9295-2f4849371542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MSG(Dataset):  \n",
    "    def __init__(self, ls_stc, truncate=False, gpt2_type=\"gpt2\", max_length=1024, added_tokens=None):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        special_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "        self.tokenizer.add_special_tokens(special_tokens)\n",
    "        if added_tokens is not None:\n",
    "            self.tokenizer.add_tokens(added_tokens)\n",
    "        self.msgs = []\n",
    "        max_len = 0\n",
    "        \n",
    "        for m in ls_stc:\n",
    "            m = torch.tensor(\n",
    "                self.tokenizer.encode(self.tokenizer.bos_token + f\"{m[:max_length]}\" + self.tokenizer.eos_token, \\\n",
    "                                      max_length=15, padding=\"max_length\", truncation=True))\n",
    "            max_len = max(len(m), max_len)\n",
    "\n",
    "            self.msgs.append(m)\n",
    "        \n",
    "        if truncate:\n",
    "            self.msgs = self.msgs[:20000]\n",
    "            \n",
    "        self.msgs_count = len(self.msgs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.msgs_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.msgs[item]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d0fc726-fcbd-4c61-855e-a6cd27c316c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_base_train = MSG(train_ls, truncate=True, gpt2_type=\"gpt2\", added_tokens=None)  \n",
    "ds_base_valid = MSG(valid_ls, truncate=True, gpt2_type=\"gpt2\", added_tokens=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b69e4f2-971d-41ef-9ebe-a7e5b282688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=ds_base_train.tokenizer,\n",
    "        mlm=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04590a6d-2c51-47ea-befb-6cc860312242",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/bayesiandater/ex_bot\",          # output directory\n",
    "    num_train_epochs=6,              # total # of training epochs\n",
    "    per_device_train_batch_size=10,  # batch size per device during training\n",
    "    per_device_eval_batch_size=10,   # batch size for evaluation\n",
    "    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    prediction_loss_only=True,\n",
    "    save_steps=1000 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "286bdd9d-2d57-44f2-b443-083859825b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_base = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model_base.resize_token_embeddings(len(ds_base_train.tokenizer))\n",
    "model_base = model_base.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd45aa8-758c-4e6b-835b-3e3b171eed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_base,                         # the instantiated ð¤ Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=ds_base_train,         # training dataset\n",
    "    eval_dataset=ds_base_valid            # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a13149f-4fb7-4860-aaab-39244f8028a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 71\n",
      "  Batch size = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.627530574798584,\n",
       " 'eval_runtime': 1.0273,\n",
       " 'eval_samples_per_second': 69.111,\n",
       " 'eval_steps_per_second': 7.787,\n",
       " 'epoch': 6.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651a3687-8834-48bb-9598-40da242e795d",
   "metadata": {},
   "source": [
    "### 1. beam search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52767d75-bc25-4226-aa03-d7fe37cdfafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it\n",
      "i love it\n",
      "i love it when people tell me what to do\n",
      "i love it i can't wait to see you\n",
      "i love it when people tell me what to do\n",
      "i love it when you dont know what to make of it\n",
      "i love it when you dont know what to make of it\n",
      "i love it i can't wait to see what you guys think\n",
      "i love it i can't wait to see what you've got in store for me\n",
      "i love it i can't wait to see what you've got\n",
      "i love it when you dont know what to make\n",
      "i love it when you can't wait to see what else you've got on your mind\n",
      "i love it when you can't wait to see what else you've got on your mind\n",
      "i love it i can't wait to see what you think about this\n",
      "i love it i can't wait to see what you guys think about\n",
      "i love it i can't wait to see what you guys think about it\n",
      "i love it i can't wait to see what you guys think about this\n",
      "i love it i can't wait to see what you think about\n",
      "i love this post\n",
      "i love it when you can't wait to see what else is happening in the world\n",
      "all water\n",
      "all water\n",
      "all water\n",
      "all water\n",
      "all watercooling\n",
      "all water now\n",
      "all watercooling\n",
      "all water in the world\n",
      "all water I\n",
      "all water in the world\n",
      "all watermarks\n",
      "all water I\n",
      "all water Advertisements\n",
      "all watercolor\n",
      "all water water\n",
      "all watercolor\n",
      "all water back\n",
      "all watercooled\n",
      "all water cool\n",
      "all water hot\n"
     ]
    }
   ],
   "source": [
    "max_length = 50 \n",
    "num_beams=20\n",
    "num_return_sequences= 20\n",
    "early_stopping= False\n",
    "no_repeat_ngram_size=2\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_base_valid):\n",
    "    ops = model_base.generate(m[:3].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                               num_beams=num_beams,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_base_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa707d-876c-4d66-bcb8-17d47dea0cf9",
   "metadata": {},
   "source": [
    "### 2. sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba7dadec-619b-455c-b5b7-9a4909e055df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love when i am the happiest i have the coolest\n",
      "i love what you are doing\n",
      "i love this i cant wait to see what you will cook it\n",
      "i love it\n",
      "i love it â¤\n",
      "i love a guest Jan\n",
      "i love\n",
      "i love the way it was meant to be so let you know\n",
      "i love the colors of this thread its like it, it came in a very humid\n",
      "i love this dude y\n",
      "i love the best\n",
      "i love, love your stuff michael klobben\n",
      "i love when they did what they do now\n",
      "i love watching wat is on\n",
      "i love red velvet\n",
      "i love thats\n",
      "i love these lil thingies\n",
      "i love it but it is free\n",
      "i love i am\n",
      "i love it\n",
      "all water\n",
      "all water\n",
      "all water\n",
      "all water\n",
      "all water\n",
      "all water\n",
      "all water, air, rain\n",
      "all water\n",
      "all water\n",
      "all water for life is in the words\n",
      "all watercool is\n",
      "all watermelon strawberry sundae\n",
      "all water buffalo\n",
      "all water\n",
      "all water\n",
      "all watercourses\n",
      "all water\n",
      "all waterfalls sky\n",
      "all water, all air, fire, rain. The world is based on\n",
      "all water\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "num_beams=20\n",
    "num_return_sequences= 20\n",
    "early_stopping=True \n",
    "no_repeat_ngram_size=2\n",
    "do_sample = True\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_base_valid):\n",
    "    ops = model_base.generate(m[:3].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                                do_sample=do_sample,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_base_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "421c9f20-502a-4507-983b-aa2cb9f8c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love this guy, im sure i have met your mom last night\n",
      "i love these guys i mean wanna know about and they dont give a fuck about dont have a dick nobody\n",
      "i love it when people ask me to do sexy pictures but i dont think so\n",
      "i love sucking dick\n",
      "i love this review\n",
      "i love this i canadian and they hate their own country\n",
      "i love watching\n",
      "i love what i see in these images and is happening\n",
      "i love it\n",
      "i love this article\n",
      "i love goodreads, but then i am also a huge sarahandrew\n",
      "i love\n",
      "i love watching it\n",
      "i love this game no it is just some simple love it\n",
      "i love this song\n",
      "i love when they have free will or if you are fat\n",
      "i love reddit.com so much\n",
      "i love\n",
      "i love it but i am going to bed\n",
      "i love sweet\n",
      "all water this is a very bad smell\n",
      "all water\n",
      "all water and dust\n",
      "all water\n",
      "all watercolour\n",
      "all water\n",
      "all water, humidity, winter\n",
      "all water, salmonella, papaya, rosemary\n",
      "all water\n",
      "all water\n",
      "all water\n",
      "all water\n",
      "all water now i guess but i just got to get in contact with some of your fav animals\n",
      "all water\n",
      "all water, the hottest oil\n",
      "all watermelon i rok i r living room, roi roan living\n",
      "all water\n",
      "all water\n",
      "all watermelon munching the wrong i am the i- is the in the i'm the jack is you your\n",
      "all watercooling\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "num_beams=20\n",
    "num_return_sequences= 20\n",
    "early_stopping=False \n",
    "no_repeat_ngram_size=2\n",
    "do_sample = True\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_base_valid):\n",
    "    ops = model_base.generate(m[:3].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                                do_sample=do_sample,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_base_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19f5f1a9-3a37-4224-ade2-375f916a6d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love itIn the\n",
      "i love itImage caption when i feel like it\n",
      "i love itThis story first aired in\n",
      "i love itA man\n",
      "i love itI\n",
      "i love it\"When we\n",
      "i love itAbout\n",
      "i love itBANGKOK (Reuters)\n",
      "i love itDescription Welcome to the perfect\n",
      "i love itWe\n",
      "all water under the stars\n",
      "all water under the sun (including sunrays)\n",
      "all water under the sun is fine\n",
      "all water under the sun\n",
      "all water under the table\n",
      "all water under the sun\n",
      "all water under the brook\n",
      "all water under the sun\n",
      "all water under the table\n",
      "all water under the table\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "num_beams=10\n",
    "num_return_sequences= 10\n",
    "early_stopping=True \n",
    "no_repeat_ngram_size=2\n",
    "do_sample = True\n",
    "temperature=0.9\n",
    "top_k = 30\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_base_valid):\n",
    "    ops = model_base.generate(m[:5].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                                do_sample=do_sample,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                top_k = top_k,\n",
    "                                temperature=temperature,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_base_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "980f7c8e-44f7-4947-b902-a91737e1228a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love itAbstract\n",
      "i love itA new anis\n",
      "i love itAs\n",
      "i love itPredictably, was the\n",
      "i love it(Reuters)\n",
      "i love itShare This The latest for me, a\n",
      "i love itGet daily updates directly\n",
      "i love itFor\n",
      "i love itA lot of\n",
      "i love itAnthropologists\n",
      "all water under the bed\n",
      "all water under the sun\n",
      "all water under the sun\n",
      "all water under the sun\n",
      "all water under the sun but i want to know what are these\n",
      "all water under the sun\n",
      "all water under the broiler\n",
      "all water under the moon\n",
      "all water under the brook\n",
      "all water under the sun hottest oil\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "num_beams=10\n",
    "num_return_sequences= 10\n",
    "early_stopping=False\n",
    "no_repeat_ngram_size=2\n",
    "do_sample = True\n",
    "temperature=0.9\n",
    "top_k = 50\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_base_valid):\n",
    "    ops = model_base.generate(m[:5].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                                do_sample=do_sample,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                top_k = top_k,\n",
    "                                temperature=temperature,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_base_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "460ce0f8-e6f1-4920-b525-c414e3122b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love itDescription: videos\n",
      "i love itI\n",
      "i love itFor it's hottest stories in science news, check out our newest addition to the science and technology\n",
      "i love itThe Last Days of the\n",
      "i love itWhen Michael.k.a. Michael Douglas is an actor and producer famous for his\n",
      "i love itAnchor of fire is great\n",
      "i love itThis article is. It's based onand well behaved\n",
      "i love itPresident Obama has\n",
      "i love itThe US economy\n",
      "i love itCrowdStrike\n",
      "all water under the sun\n",
      "all water under the sun. Please do not put cookies.\n",
      "all water under the sun\n",
      "all water under the hood\n",
      "all water under the brook\n",
      "all water under the sun in dry weather\n",
      "all water under the title The Wet Nurse\n",
      "all water under the bed\n",
      "all water under the moon\n",
      "all water under the influence of alcohol if you are feeling really\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "num_beams=10\n",
    "num_return_sequences= 10\n",
    "early_stopping=False\n",
    "no_repeat_ngram_size=2\n",
    "do_sample = True\n",
    "temperature=0.9\n",
    "top_k = 50\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_base_valid):\n",
    "    ops = model_base.generate(m[:5].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                                do_sample=do_sample,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                top_k = top_k,\n",
    "                                temperature=temperature,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_base_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1065bde-cdaf-4e74-aea0-146605f217bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e885ff4-f593-4674-b2c8-7a384f639882",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Questions:\n",
    "- Can you use GPT2 tokenizer w/o padding?\n",
    "- Hoes before bros? - check the avg sentiment with text message sentiments with different\n",
    "people\n",
    "- what is a reasonable size for the dataset for fine-tuned model to behave well enough\n",
    "- overfitting, train with added tokens for for fewer epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9771052-22cb-4ecf-b39c-78efd1ce43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import pickle\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c715ecb2-951a-4d06-98cf-e301ceac266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_from_ex = pickle.load(open(\"/Users/bayesiandater/ex_bot/resources/msg_from_ex\", \"rb\"))\n",
    "msg_from_ex = [m.lower() for m in msg_from_ex if m.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89fd7f68-8460-4a01-86ba-f4d79224bf10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(msg_from_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "974419f5-2002-4ec7-8e81-ed60a4caa3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thanks for sending it haha',\n",
       " 'i love it',\n",
       " 'lmaoooo',\n",
       " 'the grad ceremony rip',\n",
       " 'and happy grad if you are graduating !',\n",
       " 'hope you are doing well',\n",
       " 'all water under the bridge',\n",
       " 'haha itâ€™s chill dude',\n",
       " 'hvae fun!',\n",
       " 'sounds good haha',\n",
       " 'noice noice',\n",
       " 'noice noice',\n",
       " 'woah',\n",
       " 'i have too much to do at the moment',\n",
       " 'sorry',\n",
       " 'until my thesis is done',\n",
       " 'uh i am cooped at my home',\n",
       " 'fair i guess',\n",
       " 'oofs',\n",
       " 'okie will do',\n",
       " 'e commerce films??',\n",
       " 'i guess',\n",
       " 'i mean',\n",
       " 'classroom',\n",
       " 'in a',\n",
       " 'there are cameras',\n",
       " 'lmaooo',\n",
       " 'privacy concerns',\n",
       " 'ohhh',\n",
       " 'have dinner plans',\n",
       " 'uhh no eh i canâ€™t',\n",
       " 'ahh makes sense makes sense',\n",
       " 'i am an absolute saint',\n",
       " 'i was positive 1.5 months ago',\n",
       " 'as in ?',\n",
       " 'uhh not sure about regularly',\n",
       " 'i was positive already',\n",
       " 'issok',\n",
       " 'ooof',\n",
       " 'i am at the gate eh',\n",
       " 'okay okay',\n",
       " 'i am reaching utown in like 5 mins',\n",
       " 'heyyo',\n",
       " 'i will prolly come like 8ish',\n",
       " 'yesss ?',\n",
       " 'will skip dinner',\n",
       " 'i am eating too much tomorrow',\n",
       " 'yee i think nvm',\n",
       " 'so free after 7/8',\n",
       " 'my friend shifted the plan to afternoon',\n",
       " 'yup',\n",
       " 'taking it weirdly',\n",
       " 'not like',\n",
       " 'and like itâ€™s chill',\n",
       " 'yes boss gotcha haha',\n",
       " 'can cancel no worries',\n",
       " 'but i mean issok',\n",
       " 'uhh like is the coming up thingy a specific block of time ?',\n",
       " 'coz you have dinner/drinks plans now right',\n",
       " '40 % ? idk',\n",
       " 'so depends ?',\n",
       " 'wells my friend might cancel dinner and might meet her for just like afternoon chill instead',\n",
       " 'ooh will check it out',\n",
       " 'i didnâ€™t know you used short forms',\n",
       " 'oof haha issok',\n",
       " 'its just short forms are wew',\n",
       " 'haha i got it',\n",
       " 'uh ya sure can can',\n",
       " 'sat afternoon also can',\n",
       " 'uh',\n",
       " 'no biggie',\n",
       " 'can keep it flexible',\n",
       " 'oh okay sure',\n",
       " 'tryna avoid drinking two days in a row',\n",
       " 'i am drinking fri night',\n",
       " 'hmm then uh maybe not sat night',\n",
       " 'but i can possibly meet you after [11ish] if you want',\n",
       " 'i canâ€™t i have plans with a friend',\n",
       " 'yes ?',\n",
       " 'rip',\n",
       " 'oof i told my mum in the morning i am home for dinner so',\n",
       " 'sounds reliable',\n",
       " 'having it super rushed or whatever makes it aaak',\n",
       " 'yup i gets but i think we can do another time',\n",
       " 'ooh alright',\n",
       " 'we can find some other time',\n",
       " 'alrighty huh no pressure',\n",
       " 'which makes it like back and forth',\n",
       " 'by 10',\n",
       " 'and also coz i need to be back at nus',\n",
       " 'umm not tomorrow',\n",
       " 'i think i will feel weird aaaak',\n",
       " 'oof i know but uh',\n",
       " 'nvm then',\n",
       " 'i guess it be like that',\n",
       " 'oooof okay',\n",
       " 'oh okay',\n",
       " 'nope not really',\n",
       " 'so i will just figure something out nearby haha',\n",
       " 'to drink with frens',\n",
       " 'i think i have plans in uni later',\n",
       " 'uh no ah',\n",
       " 'oof',\n",
       " 'free',\n",
       " 'coz',\n",
       " 'and have spin class in the evening',\n",
       " 'ooh okay lemme see then â€” gotta figure out some dinner',\n",
       " 'so prolly wanna sleep in',\n",
       " 'hmm okay but i think i will be hungover',\n",
       " 'okay okay',\n",
       " 'oooof',\n",
       " 'wells uhhhh are you free sat evening ? i prolly have to leave by 10 but i am free from like 4/5',\n",
       " 'sounds good',\n",
       " 'ooh alright sure',\n",
       " 'oof ya i donâ€™t think i can issok',\n",
       " 'okie sure sure',\n",
       " 'we can meet up',\n",
       " 'lemme know',\n",
       " 'also if you are free this weekend by chance',\n",
       " 'yes of course',\n",
       " 'ooof',\n",
       " 'if anything i am the younger kid',\n",
       " 'i find that very hard to believe',\n",
       " 'i am the old person here',\n",
       " 'yes of course',\n",
       " 'maybe my jupyter is old rip',\n",
       " 'i didnâ€™t know that',\n",
       " 'hope you have a great day ahead',\n",
       " 'happy birthday!!',\n",
       " 'there you go',\n",
       " 'bread is good',\n",
       " 'i want bread',\n",
       " 'yes',\n",
       " 'i think i sent you everything oof',\n",
       " 'so yay',\n",
       " 'i am in a work call',\n",
       " 'sounds cool',\n",
       " 'noice i want that',\n",
       " 'for sure',\n",
       " 'i will meet god this sem',\n",
       " 'doing too much rn so issok',\n",
       " 'it be like that sometimes',\n",
       " 'rip',\n",
       " 'lmaooo',\n",
       " 'i slacked during the week',\n",
       " 'just coz there is work to do',\n",
       " 'yesss gotchaa',\n",
       " 'thenks thenks',\n",
       " 'ooof issa bopez',\n",
       " 'work takes precedence and like i was working this sunday so ya',\n",
       " 'uh go ahead coz thereâ€™s no way i can predict my next weekend lmao',\n",
       " 'then why donâ€™t you tell them that huh',\n",
       " 'adds up',\n",
       " 'lmaooo',\n",
       " 'woah really abusing ðŸ‘€ this eh',\n",
       " 'at random af timings',\n",
       " 'coz i have work meetings',\n",
       " 'uh ya so my calendar is horrible lmaoo',\n",
       " 'good morning']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_from_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c947485-ce48-4103-861e-6a8daa28e5e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(713)\n",
    "train_idx = rng.integers(low=0, high=len(msg_from_ex), size=int(0.8 * len(msg_from_ex)))\n",
    "valid_idx = [i for i in range(len(msg_from_ex)) if i not in train_idx]\n",
    "train_ls, valid_ls = np.array(msg_from_ex)[train_idx], np.array(msg_from_ex)[valid_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934ecdab-3489-43a1-89a5-df07b15043c5",
   "metadata": {},
   "source": [
    "#### decide on the number of sentence length to feed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86ab90a5-df95-41e5-bd93-879af74701d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([54., 27., 24., 14.,  3.,  3.,  0.,  1.,  0.,  1.]),\n",
       " array([ 1. ,  3.1,  5.2,  7.3,  9.4, 11.5, 13.6, 15.7, 17.8, 19.9, 22. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(m.strip().split()) for m in train_ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b5a1f9-65d7-4a86-bb89-a4ffe49bcae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the custom tokens to add to tokenizer\n",
    "# get the GPT2 token list\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "GPT2_tokens = []\n",
    "for w in tokenizer.encoder.keys():\n",
    "    w = w.strip(\"Ä \").lower()\n",
    "    GPT2_tokens.append(w)\n",
    "    \n",
    "# only add all whole words from both train + valid sets \n",
    "# that are not in the original GPT2 tokenizer's vocabulary 15\n",
    "all_words = []\n",
    "for m in msg_from_ex:\n",
    "    m = m.strip(string.punctuation).lower().split()\n",
    "    all_words += m\n",
    "\n",
    "added_tokens = []\n",
    "for word, freq in Counter(all_words).most_common():\n",
    "    if word not in GPT2_tokens:\n",
    "        added_tokens.append(word)\n",
    "        \n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d972242f-8cc0-4d00-80e3-6278b47d6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSG(Dataset):  \n",
    "    def __init__(self, ls_stc, truncate=False, gpt2_type=\"gpt2\", max_length=1024, added_tokens=None):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        special_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "        self.tokenizer.add_special_tokens(special_tokens)\n",
    "        if added_tokens is not None:\n",
    "            self.tokenizer.add_tokens(added_tokens)\n",
    "        self.msgs = []\n",
    "        \n",
    "        for m in ls_stc:\n",
    "            m = torch.tensor(\n",
    "                self.tokenizer.encode(self.tokenizer.bos_token + f\"{m[:max_length]}\" + self.tokenizer.eos_token, \\\n",
    "                                      max_length=15, padding=\"max_length\", truncation=True))\n",
    "\n",
    "            self.msgs.append(m)\n",
    "        \n",
    "        if truncate:\n",
    "            self.msgs = self.msgs[:20000]\n",
    "            \n",
    "        self.msgs_count = len(self.msgs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.msgs_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.msgs[item]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ef237b-0c73-4e24-addc-afb1d58adcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_custom_train = MSG(train_ls, truncate=True, gpt2_type=\"gpt2\", added_tokens=added_tokens)  \n",
    "ds_custom_valid = MSG(valid_ls, truncate=True, gpt2_type=\"gpt2\", added_tokens=added_tokens)  \n",
    "\n",
    "ds_base_train = MSG(train_ls, truncate=True, gpt2_type=\"gpt2\", added_tokens=None)  \n",
    "ds_base_valid = MSG(valid_ls, truncate=True, gpt2_type=\"gpt2\", added_tokens=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bcf9bd5-439f-4e73-b7f7-f2d6732be1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=ds_custom_train.tokenizer,\n",
    "        mlm=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca6469c0-585d-4ec9-9f1a-9820f20abb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/bayesiandater/ex_bot\",          # output directory\n",
    "    num_train_epochs=6,              # total # of training epochs\n",
    "    per_device_train_batch_size=10,  # batch size per device during training\n",
    "    per_device_eval_batch_size=10,   # batch size for evaluation\n",
    "    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    prediction_loss_only=True,\n",
    "    save_steps=1000 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53d3269b-0ac2-4caa-a3bd-7384a65147ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_custom = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model_custom.resize_token_embeddings(len(ds_custom_train.tokenizer))\n",
    "model_custom = model_custom.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e994a-6e83-494c-8d43-013967b1261d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_custom,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=ds_custom_train,         # training dataset\n",
    "    eval_dataset=ds_custom_valid            # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "190f2914-2c6c-482f-86e2-7e7591872daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 71\n",
      "  Batch size = 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 7.913658618927002,\n",
       " 'eval_runtime': 1.1015,\n",
       " 'eval_samples_per_second': 64.46,\n",
       " 'eval_steps_per_second': 7.263,\n",
       " 'epoch': 6.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf0fd4-831c-4806-a168-dd65b001c449",
   "metadata": {},
   "source": [
    "### 1. greedy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bad9af5-4004-486b-93b2-e338d255f9ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "i love this post\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all water\n",
      " yee \n",
      "\n",
      "all water\n",
      " yee \n",
      " yee\n",
      "2\n",
      "haha\n",
      " yee\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sounds.sound\n",
      " yee sound\n",
      " yee sound\n",
      " yee sound\n",
      " yee sound\n",
      " yee sound\n",
      "\n",
      "sound\n",
      "4\n",
      "noice noice  i-cricket.com\n",
      " yee\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry 4/5 \n",
      "\n",
      "The following is a prolly \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      "\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fair i.imgur. bopez.png\n",
      " yee.png\n",
      " yee.png\n",
      " yee.png\n",
      " yee. yee \n",
      "\n",
      "i.imgur. i.imgur yee.png\n",
      " yee.png\n",
      " yee. yee \n",
      " yee.png\n",
      "\n",
      "7\n",
      "oofs 4/5 \n",
      "\n",
      "The New York Times\n",
      " yee\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okie williams.com\n",
      "9\n",
      "ahh makeshift_tutorials\n",
      "10\n",
      "i am\n",
      " yee \n",
      " yee\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as in\n",
      " yee\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uhh notify me\n",
      " yee notify me\n",
      " yee not yee notify me\n",
      " yee not yee notify me\n",
      "\n",
      "I'm not a stranger\n",
      "13\n",
      "i was a guest yee\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issok 4/5 \n",
      "\n",
      "The New York Times\n",
      " yee\n",
      "15\n",
      "ooof 4/5 \n",
      "\n",
      "The New York Times\n",
      " yee\n",
      "16\n",
      "okay\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am\n",
      " yee \n",
      " yee\n",
      "18\n",
      "heyyo 4/5 \n",
      "\n",
      "The New York Times\n",
      " yee\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i will be able to read\n",
      " yee\n",
      "20\n",
      "yee i\n",
      " yee  j\n",
      " yee \n",
      " yee\n",
      "21\n",
      "so free\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yup 4/5 \n",
      "\n",
      "The New York Times\n",
      " yee\n",
      "23\n",
      "taking it 4/5\n",
      "24\n",
      "coz you 4/5\n",
      "25\n",
      "ooh williams.com\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i didnâ€™t  james-michael-s-and- 4/5  james-m yee  james-m\n",
      "27\n",
      "its just 4/5 \n",
      " yee\n",
      "28\n",
      "sat afternoon\n",
      " yee\n",
      "29\n",
      "no biggie \n",
      " 4/5\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh okay\n",
      "31\n",
      "i am\n",
      " yee \n",
      " yee\n",
      "32\n",
      "yes?\n",
      " yee\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oof ios_ 4/5\n",
      "34\n",
      "yup i.imgur. yee.imgur.\n",
      "35\n",
      "alrighty honey\n",
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which makes jupyter 4/5 \n",
      " yee\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and also 4/5 4/5 prolly 4/5 4/5  -1\n",
      " yee  -1\n",
      " yee  -1\n",
      " yee  -1\n",
      " yee  -1\n",
      " yee  -1\n",
      " yee  -1\n",
      " yee  -1\n",
      " yee  -1\n",
      " yee  -1\n",
      " yee  -1\n",
      "38\n",
      "i think it's cool to be a dick\n",
      " yee\n",
      "39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvm then\n",
      " yee\n",
      "40\n",
      "oooof ok\n",
      "41\n",
      "oh okay\n",
      "42\n",
      "nope\n",
      " yee\n",
      "43\n",
      "so i 4/5\n",
      "44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uh no\n",
      " yee\n",
      "45\n",
      "coz 4/5 \n",
      "\n",
      "The Best of prolly \n",
      " yee\n",
      "46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and have 4/5 jupyter prolly 4/5 4/5 \n",
      " yee \n",
      "\n",
      "\"\n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      " yee \n",
      "\n",
      "47\n",
      "okay\n",
      "48\n",
      "okie sure\n",
      "49\n",
      "yes of course\n",
      " yee\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ooof 4/5 \n",
      "\n",
      "The New York Times\n",
      " yee\n",
      "51\n",
      "yes of course\n",
      " yee\n",
      "52\n",
      "hope_to_ 4/5\n",
      "53\n",
      "there you are\n",
      "54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bread is a prolly \n",
      " yee\n",
      "55\n",
      "i want to know more\n",
      "56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes 4/5 \n",
      "\n",
      "The New York Times\n",
      " yee\n",
      "57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noice i-am-online-online-online-online-online-online-online-online-online-online-online-online-online-online-online-online-online-online-online-online-online-online-\n",
      "58\n",
      "for sure\n",
      " yee\n",
      "59\n",
      "doing too much\n",
      " yee\n",
      "60\n",
      "it be\n",
      " yee\n",
      "61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmaooo 4/5 \n",
      "\n",
      "The New York Times\n",
      " yee\n",
      "62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just coz.\n",
      " yee.\n",
      "\n",
      "I prolly.\n",
      " yee.\n",
      " yee.\n",
      " yee.\n",
      " yee.\n",
      " yee yee.\n",
      " yee oooof.\n",
      " yee yee yee.\n",
      " yee yee.\n",
      " yee yee yee.\n",
      " yee yee ooh\n",
      "63\n",
      "yesss gotchaa  i-cricket.com\n",
      " yee\n",
      "64\n",
      "ooof issa\n",
      "65\n",
      "uh go\n",
      "66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adds\n",
      " yee\n",
      "67\n",
      "woah reallygoodguy\n",
      " yee \n",
      " yee\n",
      "68\n",
      "at random\n",
      " yee\n",
      "69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coz i.imgur. 4/5.imgur.\n",
      "70\n",
      "uh ya 4/5\n"
     ]
    }
   ],
   "source": [
    "max_length = 50 \n",
    "\n",
    "for j, m in enumerate(ds_custom_valid):\n",
    "    print(j)\n",
    "    ops = model_custom.generate(m[:3].unsqueeze(0), max_length = max_length)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        print(ds_custom_train.tokenizer.decode(beam, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6158c710-b946-4e90-b62d-b66d3ef0eeef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love this post\n",
      "all water\n",
      " ooh \n",
      "all water\n",
      " ooh \n",
      " ooh\n",
      "haha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sounds.sound\n",
      " ooh\n",
      "noice noice \n",
      " aaaak \n",
      "\n",
      "The first yup \n",
      " ooh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry aaaak \n",
      "\n",
      "The following is a yup \n",
      " ooh \n",
      " ooh \n",
      " ooh \n",
      " ooh \n",
      " ooh\n",
      "fair i aaaak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oofs aaaak \n",
      "\n",
      "The following is a yup \n",
      " ooh \n",
      " ooh \n",
      " ooh \n",
      " ooh \n",
      " ooh \n",
      " ooh \n",
      " ooh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okie williams.com\n",
      " ooh williams.com\n",
      " ooh williams.com\n",
      " ooh will ooh williams ooh\n",
      "ahh makes\n",
      " ooh \n",
      "\n",
      "This is a very yup \n",
      " ooh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am_pizza_dummy\n",
      " ooh\n",
      "as in\n",
      " ooh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uhh notify me yup \n",
      " ooh \n",
      "\n",
      "I yup \n",
      " ooh \n",
      " oofs \n",
      "\n",
      "I\n",
      "i was a guest yup\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issok aaaak \n",
      "\n",
      "The following is a yup \n",
      " ooh \n",
      " ooh \n",
      " ooh \n",
      " ooh \n",
      " ooh \n",
      " ooh \n",
      " ooh\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m ds_custom_valid:\n\u001b[0;32m----> 4\u001b[0m     ops \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_custom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, beam \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ops):\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(ds_custom_train\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(beam, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/acoustic0/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/acoustic0/lib/python3.8/site-packages/transformers/generation_utils.py:1348\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1344\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when doing greedy search.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1345\u001b[0m         )\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;66;03m# 10. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_sample_gen_mode:\n\u001b[1;32m   1361\u001b[0m     \u001b[38;5;66;03m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1362\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1363\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1364\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1368\u001b[0m         renormalize_logits\u001b[38;5;241m=\u001b[39mrenormalize_logits,\n\u001b[1;32m   1369\u001b[0m     )\n",
      "File \u001b[0;32m~/acoustic0/lib/python3.8/site-packages/transformers/generation_utils.py:1742\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1739\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 1742\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   1750\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/acoustic0/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/acoustic0/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1046\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1046\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/acoustic0/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/acoustic0/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:889\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    879\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    880\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    881\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/acoustic0/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/acoustic0/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:426\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    425\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 426\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    428\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/acoustic0/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/acoustic0/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:353\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 353\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m    355\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[0;32m~/acoustic0/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/acoustic0/lib/python3.8/site-packages/transformers/pytorch_utils.py:112\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    111\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_length = 50 \n",
    "\n",
    "for m in ds_custom_valid:\n",
    "    ops = model_custom.generate(m[:3].unsqueeze(0), max_length = max_length)\n",
    "    for i, beam in enumerate(ops):\n",
    "        print(i, ds_custom_train.tokenizer.decode(beam, skip_special_tokens=True))\n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c8ac8-0578-4669-9703-ed0542bf0fc7",
   "metadata": {},
   "source": [
    "### 2. beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e6f7046-0c41-439f-9a26-ab8602f2fc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it yee\n",
      "i love it\n",
      "i love this song yee\n",
      "i love this post yee\n",
      "i love this photo yee\n",
      "i love this topic yee\n",
      "i love this thread yee\n",
      "i love this post\n",
      "i love this review yee\n",
      "i love yee\n",
      "i love this comment yee\n",
      "i love this site yee\n",
      "i love this comment\n",
      "i love this blog yee\n",
      "i love\n",
      "i love this topic\n",
      "i lovevideo.com\n",
      "i love this item yee\n",
      "i love it when you\n",
      "i love this blog\n",
      "all water yee\n",
      "all water 4/5 yee\n",
      "all water yee water yee\n",
      "all water yee yee\n",
      "all water jupyter yee\n",
      "all water\n",
      "all watercolor yee\n",
      "all water yee Advertisements\n",
      "all water 4/5 yee\n",
      "all water yee water\n",
      "all water yee yee\n",
      "all water 7/8 yee\n",
      "all water yee\n",
      "all water jupyter yee\n",
      "all water yee all water\n",
      "all water yee\n",
      "all water yee back yee\n",
      "all water 4/5\n",
      "all watercolor\n",
      "all water, jupyter\n"
     ]
    }
   ],
   "source": [
    "# beam search\n",
    "max_length = 50 \n",
    "num_beams=20\n",
    "num_return_sequences= 20\n",
    "early_stopping=True \n",
    "\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_custom_valid):\n",
    "    ops = model_custom.generate(m[:3].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                               num_beams=num_beams,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_custom_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "791d3b1d-2143-41a1-9d7c-7b23e51f07f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it yee\n",
      "i love it\n",
      "i love this song yee\n",
      "i love this post yee\n",
      "i love this photo yee\n",
      "i love this topic yee\n",
      "i love this thread yee\n",
      "i love this post\n",
      "i love this review yee\n",
      "i love yee\n",
      "all water yee\n",
      "all water 4/5 yee\n",
      "all water yee water yee\n",
      "all water yee yee\n",
      "all water jupyter yee\n",
      "all water\n",
      "all watercolor yee\n",
      "all water yee Advertisements\n",
      "all water 4/5 yee\n",
      "all water yee water\n"
     ]
    }
   ],
   "source": [
    "# beam search\n",
    "max_length = 50 \n",
    "num_beams=20\n",
    "num_return_sequences= 10\n",
    "early_stopping=True \n",
    "\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_custom_valid):\n",
    "    ops = model_custom.generate(m[:3].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                               num_beams=num_beams,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_custom_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f4135b-22b6-4bf1-b39e-937e30971f19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it ooh\n",
      "i love it\n",
      "i love this topic ooh\n",
      "i love this thread ooh\n",
      "i love this post ooh\n",
      "i love this song ooh\n",
      "i love\n",
      "i love this post\n",
      "i love ooh\n",
      "i love this ooh\n",
      "i love this item ooh\n",
      "i love this topic\n",
      "i love this story ooh\n",
      "i love this blog ooh\n",
      "i love this comment\n",
      "i love this place ooh\n",
      "i love this thread\n",
      "i love it when you\n",
      "i love this song\n",
      "i love this story\n",
      "all water ooh\n",
      "all water ooh water ooh\n",
      "all water yup ooh\n",
      "all water ooh ooh\n",
      "all water hungover ooh\n",
      "all water ooh ooh\n",
      "all water\n",
      "all water aaaak ooh\n",
      "all water thingy ooh\n",
      "all water lmaoo ooh\n",
      "all water ooh water\n",
      "all water ooh all water\n",
      "all water yup ooh\n",
      "all water ooh\n",
      "all water yup\n",
      "all water ooh All water\n",
      "all water yup\n",
      "all water hungover\n",
      "all water aaaak\n",
      "all water thingy\n"
     ]
    }
   ],
   "source": [
    "# beam search\n",
    "max_length = 50 \n",
    "num_beams=20\n",
    "num_return_sequences= 20\n",
    "early_stopping=True \n",
    "\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_custom_valid):\n",
    "    ops = model_custom.generate(m[:3].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                               num_beams=num_beams,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_custom_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45cd25-795b-48af-aaaf-c6089339ab3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it ooh\n",
      "i love it\n",
      "i love this topic ooh\n",
      "i love this thread ooh\n",
      "i love this post ooh\n",
      "i love this song ooh\n",
      "i love\n",
      "i love this post\n",
      "i love ooh\n",
      "i love this ooh\n",
      "i love this item ooh\n",
      "i love this topic\n",
      "i love this story ooh\n",
      "i love this blog ooh\n",
      "i love this comment\n",
      "i love this place ooh\n",
      "i love this thread\n",
      "i love it when you\n",
      "i love this song\n",
      "i love this story\n",
      "all water ooh\n",
      "all water yup ooh\n",
      "all water ooh ooh\n",
      "all water hungover ooh\n",
      "all water\n",
      "all water aaaak ooh\n",
      "all water thingy ooh\n",
      "all water lmaoo ooh\n",
      "all water ooh water\n",
      "all water yup ooh\n",
      "all water ooh\n",
      "all water yup\n",
      "all water hungover ooh\n",
      "all water yup\n",
      "all water aaaak ooh\n",
      "all water thingy ooh\n",
      "all water lmaoo ooh\n",
      "all water hungover\n",
      "all water aaaak\n",
      "all water thingy\n"
     ]
    }
   ],
   "source": [
    "max_length = 50 \n",
    "num_beams=20\n",
    "num_return_sequences= 20\n",
    "early_stopping=True \n",
    "no_repeat_ngram_size=2\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_custom_valid):\n",
    "    ops = model_custom.generate(m[:3].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                               num_beams=num_beams,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_custom_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ca073-20dc-496b-acba-09bb09980898",
   "metadata": {},
   "source": [
    "### 3. sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f3fb1bb9-f734-4bf2-804d-f1b795e589ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love this piece?\n",
      "i lovevideo\n",
      "i love my life so much\n",
      "i love and dont get married this site\n",
      "i love redbox okie\n",
      "i love this book, it's essential reading for anyone needing a ahh\n",
      "i love this thread\n",
      "i love tryna new\n",
      "i love it\n",
      "i love it\n",
      "i love sweethearts\n",
      "i love this post\n",
      "i love pink\n",
      "i love life\n",
      "i love it\n",
      "i love this song okie\n",
      "i love when i find my skin dry but i am allergic to this i dont smoke it\n",
      "i love these beads... yee\n",
      "i love this article\n",
      "i love_yourbasket okie _dishwasher _faux_pork_crying_door gotcha _flat_nose ðŸ‘€ is_dog_tongue\n",
      "all watercolor ink and sparkling ink, you will need to get the item you want.\n",
      "all water : water hot water lhb okie hot cold water hungover hot yee cold cold 4/5 hot salt water mh\n",
      "all water okie the_water\n",
      "all watercolor\n",
      "all watercolors jupyter okie heyyo\n",
      "all water.water.your. canâ€™t. your. okie that\n",
      "all water-color\n",
      "all water in your drink okie\n",
      "all water water color\n",
      "all water okie\n",
      "all water (8 x 10) okie all\n",
      "all water\n",
      "all watercolor\n",
      "all watercolor all\n",
      "all water okie water\n",
      "all water\n",
      "all water_top { display: none; }\n",
      "all watermel 4/5\n",
      "all wateris water\n",
      "all watermelon thingy -gardened -garlandia\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "num_beams=20\n",
    "num_return_sequences= 20\n",
    "early_stopping=True \n",
    "no_repeat_ngram_size=2\n",
    "do_sample = True\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_custom_valid):\n",
    "    ops = model_custom.generate(m[:3].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                                do_sample=do_sample,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_custom_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41b4319a-ad70-4487-8fbe-e54dc24c1e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love this post\n",
      "i love the way i wish it was possible\" okie\n",
      "i love this site okie\n",
      "i love watching adobe sexy okie\n",
      "i love this i dont gotcha i will never forget okie\n",
      "i love this shit\n",
      "i love >>\n",
      "i lovevideo.video okie\n",
      "i love watching this video\n",
      "i love\n",
      "i love this post(s)\n",
      "i love it okie\n",
      "i love white bread butiyyur, my_cupcake i love hotcakes, hotbread\n",
      "i love that how come its not a baby. okie it\n",
      "i love it\n",
      "i love watching ralphs\n",
      "i love https://www\n",
      "i love to see\n",
      "i love this product\n",
      "i love this topic\n",
      "all water_color.png gotchaa orew.jpg\n",
      "all water, l yay , b c â€¢\n",
      "all water okie all fire gotchaa\n",
      "all water milk yee wine okie\n",
      "all water\n",
      "all water no splash v4 all waterno splash\n",
      "all water okie no ice 0 gotcha\n",
      "all water okie t 4/5 itâ€™s all my\n",
      "all water, wind,\n",
      "all watercolour the last word okie\n",
      "all water1 the water2 theWater theWisemore. ooof THEWISMOROTHTHEWOROOM yee thewispolicyos.\n",
      "all water\n",
      "all watercolor\n",
      "all watermark 4/5\n",
      "all water is not coz the one who gotchaa\n",
      "all waterisland bopez yee\n",
      "all water :, okie gotcha\n",
      "all water\n",
      "all water = watercooled max_temp_dir_maximal frens _cache_default\n",
      "all water\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "num_beams=20\n",
    "num_return_sequences= 20\n",
    "early_stopping=True \n",
    "no_repeat_ngram_size=2\n",
    "do_sample = True\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_custom_valid):\n",
    "    ops = model_custom.generate(m[:3].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                                do_sample=do_sample,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                early_stopping=False)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_custom_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a39c4d7e-baeb-4d70-91c2-dc69777bb2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it issok 1.5 uhh\n",
      "i love it\n",
      "i love it 1.5\n",
      "i love it heyyo 1.5 yee lmaooo uhh dinner/drinks\n",
      "i love it uhh dinner/drinks a 1.5 lmaooo yee\n",
      "i love it lmaooo uhh yee prolly\n",
      "i love it heyyo 1.5 Share lmao lmaooo\n",
      "i love it coz\n",
      "i love it coz\n",
      "i love it issok\n",
      "all water under the sun yee\n",
      "all water under the sun 1 cup donâ€™t\n",
      "all water under the moon uhh\n",
      "all water under the sun, rain, sunshine, storms 1.5\n",
      "all water under the broiler 1.5\n",
      "all water under the sun at noon or weirdly\n",
      "all water under the sun water is a shade under. yee\n",
      "all water under the sun yee\n",
      "all water under the sun lmaooo uhh\n",
      "all water under the hood\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "num_beams=10\n",
    "num_return_sequences= 10\n",
    "early_stopping=True \n",
    "no_repeat_ngram_size=2\n",
    "do_sample = True\n",
    "temperature=0.9\n",
    "top_k = 20\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_custom_valid):\n",
    "    ops = model_custom.generate(m[:5].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                                do_sample=do_sample,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                top_k = top_k,\n",
    "                                temperature=temperature,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_custom_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1caf960-de2d-409e-9d79-60a6725360bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it tryna\n",
      "i love it idk lily liana ro yup\n",
      "i love it cnn\"\n",
      "i love it uhh the way things are and lmaooo the heyyo\n",
      "i love it issok vid dev en espa lmaooo\n",
      "i love it jupyter\n",
      "i love it issok pimping\n",
      "i love it coz\n",
      "i love it issok 1.5\n",
      "i love it lmaooo 1.5 uhh\n",
      "all water under the sun\n",
      "all water under the table for indoor plumbing yee\n",
      "all water under the bed\n",
      "all water under the table \\ heyyo\n",
      "all water under the sun yee uhh okie\n",
      "all water under the moon\n",
      "all water under the sun and dust rain snow dinner/drinks okie\n",
      "all water under the sun the water that is heyyo\n",
      "all water under the covers yay\n",
      "all water under the sun\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "num_beams=10\n",
    "num_return_sequences= 10\n",
    "early_stopping=False\n",
    "no_repeat_ngram_size=2\n",
    "do_sample = True\n",
    "temperature=1\n",
    "top_k = 50\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_custom_valid):\n",
    "    ops = model_custom.generate(m[:5].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                                do_sample=do_sample,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                top_k = top_k,\n",
    "                                temperature=temperature,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_custom_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c4da85-e29e-44c9-bbe5-46dcfd7f1d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it jupyter lmaooo\n",
      "i love it coz prolly 1.5 yee The Last of Us yup\n",
      "i love it slacked\n",
      "i love it lmaooo\n",
      "i love it noice\n",
      "i love it issok\n",
      "i love it issok\n",
      "i love it\n",
      "i love it heyyo 1.5\n",
      "i love it heyyo\n",
      "all water under the hood.\n",
      "all water under the sun, rain or shine 1.5\n",
      "all water under the sun lmaooo hot air dinner/drinks hot sauce 1.5\n",
      "all water under the sun yee all rain no rain dont forget\n",
      "all water under the table\n",
      "all water under the sun 1.5\n",
      "all water under the sun dinner/drinks\n",
      "all water under the sun yee\n",
      "all water under the table\n",
      "all water under the sun, uhh\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "num_beams=10\n",
    "num_return_sequences= 10\n",
    "early_stopping=False\n",
    "no_repeat_ngram_size=2\n",
    "do_sample = True\n",
    "temperature=0.9\n",
    "top_k = 10\n",
    "total = []\n",
    "\n",
    "for j, m in enumerate(ds_custom_valid):\n",
    "    ops = model_custom.generate(m[:5].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                                do_sample=do_sample,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                top_k = top_k,\n",
    "                                temperature=temperature,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_custom_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af26433-e83d-43af-8de6-80adee605366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open-ended generation\n",
    "class MSG2(Dataset):  \n",
    "    def __init__(self, ls_stc, truncate=False, gpt2_type=\"gpt2\", max_length=1024, added_tokens=None):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        special_tokens = {'pad_token':self.tokenizer.eos_token ,'sep_token':'<|sep|>'}\n",
    "        self.tokenizer.add_special_tokens(special_tokens)\n",
    "        if added_tokens is not None:\n",
    "            self.tokenizer.add_tokens(added_tokens)\n",
    "        self.msgs = []\n",
    "        max_len = 0\n",
    "        \n",
    "        for m in ls_stc:\n",
    "            m = torch.tensor(\n",
    "                self.tokenizer.encode(self.tokenizer.bos_token + f\"{m[:max_length]}\" + self.tokenizer.eos_token, \\\n",
    "                                      max_length=15, padding=\"max_length\", truncation=True))\n",
    "            max_len = max(len(m), max_len)\n",
    "\n",
    "            self.msgs.append(m)\n",
    "        \n",
    "        if truncate:\n",
    "            self.msgs = self.msgs[:20000]\n",
    "            \n",
    "        self.msgs_count = len(self.msgs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.msgs_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.msgs[item]\n",
    "\n",
    "ds_custom_valid2 = MSG2(valid_ls, truncate=True, gpt2_type=\"gpt2\", added_tokens=added_tokens)  \n",
    "\n",
    "max_length = 100\n",
    "num_beams=10\n",
    "num_return_sequences= 10\n",
    "early_stopping=False\n",
    "no_repeat_ngram_size=2\n",
    "do_sample = True\n",
    "temperature=0.9\n",
    "top_k = 10\n",
    "\n",
    "for j, m in enumerate(ds_custom_valid2):\n",
    "    ops = model_custom.generate(m[:5].unsqueeze(0), \n",
    "                                max_length = max_length,\n",
    "                                do_sample=do_sample,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                                top_k = top_k,\n",
    "                                temperature=temperature,\n",
    "                                early_stopping=early_stopping)\n",
    "    \n",
    "    for i, beam in enumerate(ops):\n",
    "        o = ds_custom_train.tokenizer.decode(beam, skip_special_tokens=True)\n",
    "        o = re.sub(r\"[ +|\\n]\", \" \", o).strip(\"\\n \")\n",
    "        o = re.sub(r\" +\", \" \", o).strip(\"\\n \")\n",
    "        if o not in [\"\",  \" \"]:\n",
    "            print(o)\n",
    "            total.append(o)\n",
    "            \n",
    "    if j == 1:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ba5ed690-6db9-483b-9698-237138b72254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it bopez _museum_lose_your_ idk _my_life_ yee _numbers_of_the_century _new_facts if ( _check_empty()) okie _is_not_a_true gotchaa _no_truth itâ€™s _universe gotcha _ bopez stories timings _\n",
      "i love it lmaoo\n",
      "i love it lmaoo\n",
      "i love it gotchaa tryna okie â€¢ gotcha okie gotcha\n",
      "i love it canâ€™t\n",
      "i love it bopez\n",
      "i love it 7/8\n",
      "i love it lmaoo\n",
      "i love it bopez\n",
      "i love it lmao okie\n",
      "all water under the sun okie\n",
      "all water under the table\n",
      "all water under the moon okie\n",
      "all water under the bed_lose_ gotchaa\n",
      "all water under the sun\n",
      "all water under the sun tryna I hungover i okie\n",
      "all water under the sun okie\n",
      "all water under the table.\n",
      "all water under the sun\n",
      "all water under the table\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0d40830-418a-4248-a9f6-9636006ead19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
